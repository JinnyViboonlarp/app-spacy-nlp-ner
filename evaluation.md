# Evaluating Named Entity Recognition

Current situation:

- One hundred random entities were extracted from the results of parsing 20 NewsHour transcripts
  - Only used the following entity types: PERSON, LOC, ORG, GPE and NORP
- These were put in a spreadsheet with the entity type and a count of how often each entity occurred
  - There was no clustering of variantrs of the same entity
- Annotators checked for each entity whether it was an entity and whether the entity type was correct
- This gave of entity precision numbers on the type level (not the token level)
- For recall we used some simple heuristics to find all potential entities, then had annotators check whether these were entities and what their types are. These could be used to calculate recall of spaCy NER.

Plan is to move this to a more classical precision and recall evaluation on entity tokens (that is, using all instances in the text) on a variety of data using gold standards. 

More precisely:

- Transcripts are generated for some videos.
  - Transcripts can be of three types:
    - manually annotated.
    - automatically generated by Kaldi.
    - automatically generated by Kaldi, but followed by manual curation by FixIt.
  - For comparison reasons we should have some videos where transcripts are generated in all three ways above.
- We will select fragments of transcripts, with the following requirements:
  - Should come from more than one program, starting with transcripts from the NewsHour.
  - Should be of all three types.
  - Should be likely to have at least some entities (we do not want to slog through large pieces of text that have no entities).
  - Should have at least a couple of hundred entities, distribution to be determined.
- All selected transcripts will be manually annotated with a standard tool.
  - All spans that are entities
    - Types of entities to annotate is to be determined, but most likely to set is similar to what we used before (PERSON, LOC, ORG, GPE and NORP).
  - For each annotated span we add the entity type.
  - Annotation guidelines to be adapted from existing entity annotation tasks.
- Write scripts for automatic evaluation of an NER tool running on the gold data
  - Does not necessarily need to be within the CLAMS framework, but it is probaly a good idea to do that.
 - Write guidelines on how to add new annotated data given a new data set

The above focused on evaluation entities in speech, we also need data on entities recognized via OCR. Some notes on this:

- We will assume just one kind of data, text boxes recognized EAST/Tesseract and OCR-ed by Tesseract.
  - Probably keep information on whether the box was within a slate or rolling credit.
- Randomly select some amount of them.
- Annotation
  - Should ideally be multimodal, that is, both the text and the image in the video are shown, the text is annotated but the image is shown as context.

