# Evaluating Named Entity Recognition

Focused on spaCy NER but also relevant to other NER modules.

### Current situation

We used an informal evaluation measuring precision on a set of entity types (not tokens, so there would be only one data point for each set of identical named entities).

- One hundred random entities were extracted from the results of parsing 20 NewsHour transcripts
  - Only used the following entity types: PERSON, LOC, ORG, GPE and NORP
- These were put in a spreadsheet with the entity type and a count of how often each entity occurred
  - There was no clustering of variantrs of the same entity
- Annotators checked for each entity whether it was an entity and whether the entity type was correct

For recall we used some simple heuristics to find all potential entities (using capitalized sequences of words), then had annotators check whether these were entities and what their types are. Those entities and their annotations were used to calculate recall of spaCy NER.

### Evaluation plan

We want to move to a more classical precision and recall evaluation on entity tokens (that is, using all instances in the text) on a variety of data using gold standards.

More precisely:

- Transcripts are generated for some videos.
  - Transcripts can be of three types:
    - manually annotated.
    - automatically generated by Kaldi.
    - automatically generated by Kaldi, but followed by manual curation by FixIt.
  - For comparison reasons we should have some videos where transcripts are generated in all three ways above.
- We will select fragments of transcripts, with the following requirements:
  - Should come from more than one program, starting with transcripts from the NewsHour.
  - Should be of all three types.
  - Should be likely to have at least some entities (we do not want to slog through large pieces of text that have no entities).
  - Should have at least a couple of hundred entities, distribution to be determined.
- All selected transcripts will be manually annotated with a standard tool.
  - All spans that are entities
    - Types of entities to annotate is to be determined, but most likely the set is similar to what we used before (PERSON, LOC, ORG, GPE and NORP).
  - For each annotated span we add the entity type.
  - Annotation guidelines to be adapted from existing entity annotation tasks.
- Write scripts for automatic evaluation of an NER tool running on the gold data
  - Does not necessarily need to be within the CLAMS framework, but it is probaly a good idea to do that.
 - Write guidelines on how to add new annotated data given a new data set

#### Evaluating entities generated by OCR

The above focused on evaluation entities in speech, we also need data on entities recognized via OCR. Some notes on this:

- We will assume just one kind of data, text boxes recognized EAST/Tesseract and OCR-ed by Tesseract.
  - Probably keep information on whether the box was within a slate or rolling credit.
- Randomly select some amount of them.
- Annotation
  - Should ideally be multimodal, that is, both the text and the image in the video are shown, the text is annotated but the image is shown as context.

#### Evaluating entity linking

For each dataset we also need to include linking annotation for at least a subset of the transcripts. This means an additional property in the annotation whose content is at first just a link to WikiData [https://en.wikipedia.org/wiki/Wikidata](https://en.wikipedia.org/wiki/Wikidata) or DBPedia [https://en.wikipedia.org/wiki/DBpedia](https://en.wikipedia.org/wiki/DBpedia).

